# JBNU 2025-1 Mobile Programming
Mobile Prgramming with Kotlin(Android Studio), Python 3

이 리포지토리는 전북대학교 2025학년도 1학기 모바일프로그래밍 프로젝트 진행상황을 기록한 리포지토리입니다.
---
## 프로젝트 요약
### 프로젝트명: BIS 기반 도착예측 및 대중교통 길찾기 개선 앱
#### 프로젝트 목표
> 1. 대한민국 버스 시스템 한계로 인한 버스의 정시성 부족 문제를 간접적으로 해결
> 2. 기존 네이버/카카오지도 앱보다 더 정확한 대중교통 길찾기 AI와 유저 맞춤형 예측을 제공
> 3. 딥러닝 모델 기반의 도착 시간 예측 + 실시간 알림 + 사용자 피드백 반영

### 전체 시스템 구조
#### 데이터 수집
> 1. 전주시 BIS API 활용: 정류장/노선 정보, 실시간 운행정보, 배차간격 등 조사
> 2. 날씨 API: 시간/위치 기반 날씨 정보 수집 (nx ny grid 기준으로, )
> 3. 교통 흐름 데이터: 전주시교통정보센터 비공식 API 기반
> PostgreSQL을 활용하여 데이터베이스를 구성하고, 위에서 모은 데이터를 기반으로 전처리 - 학습 예정.
#### 데이터 전처리 및 통합
> 정류장 ID, 노선 번호, 요일(평일/주말/공휴일), 버스 배차시간, 실제 정류장 도착 시간, 날씨, 교통혼잡도 등을 정규화하여 테이블 생성
#### 모델 설계 및 학습
> AI모델 예측 목표 : 정류장 A에, W요일 X시 Y분 Z날씨일 때 출발한 B번 버스가 실제로 언제 도착하는가?
> 사용이 예상되는 AI 기술
> 1. 데이터 기반 1차 추론: 회귀 기반 MLP, XGBoost
> 2. 보정 방식: 단순 선형 보간, LSTM 기반 시계열 지연 패턴 학습
> 3. 2차 추론: 실시간 피드백 기반 재보정 알고리즘
> 4. Pytorch기반 Continual Learning (ex. Experience Replay)
> 5. 보조 분석: k-NN 기반 유사 노선/날씨 탐색 후 평균 지연값 활용 가능
#### 백엔드 (서버/AI)
> 1. Python + Flask
> 2. 모델 서버와의 통신
> 3. DB 연동 (PostgreSQL)
> 4. 알림 시스템 처리
#### 프론트엔드 (Android/iOS(차후))
> 1. 출발/도착 정류장 입력 (또는 출발/도착지 입력으로 도보까지 계산)
> 2. 예상 도착시간 출력
> 3. (선택) 사용자 피드백 입력: 실제로 버스가 언제 도착했는지
> 4. 알림 기능 포함 (슬슬 지금쯤 나가야 한다.)

### 세부 기능 계획
1. 실시간 위치 확인: GPS와 API를 통한 실시간 위치 받아오기 - requests, BIS API
2. 도착 예측: DB 기반 도착시간을 예측 - PyTorch, 회귀모델
3. 사용자 피드백 수집: 실제 도착시간 입력 - PostgreSQL (DB에 insert하는 방식...)
4. 사용자 알림: 출발해야 하는 시간 알림 등
5. 대체경로 추천: 지연 시 다른 노선 제안 - 강화학습 / 데이크스트라

#### 노트
1. 고령자/시각장애인 배려로 음성-진동 기반 알링 기능을 넣어도 될듯?
2. 실제 도착시간을 입력하는 시스템을 "실시간 BIS로 정보를 수신하다가 버스가 목표 정류장에 도착했다고 판단되는 시간에 알림으로 보냄"
---
## 중간 정리 (2025-04-22)
### 전체 시스템 목표
> 사용자가 도착 시간 또는 출발 시간만 입력하면,
> AI기반 예측과 실시간 교통 흐름을 반영하여
> 최적 경로와 출발 타이밍을 안내해주는 교통 앱

### 시스템 구성 요약
1. 예측 AI : 버스의 정류장별 도착 시간 예측 (LSTM, GRU, XGBoost 등)
2. 시간표 생성기 : AI 추론 결과로 각 정류장별 예측 시간표 생성
3. 실시간 보정기 : 버스가 움직일 때 위치 추적을 기반으로 실시간 시간표를 보정 (버스 위치, 날씨, 교통혼잡도 기반)
4. 경로 추론기 : 예측 시간표 기반 A* 탐색 (시간 가중치 + 휴리스틱)
5. 알림 시스템 : 출발 시점 알림 및 경로 재탐색 (도보 시간 포함하여 조건 판단)
6. 안드로이드 UI : 지도 연동 및 사용자 인터랙션

### 시간표 기반 경로 탐색 전략
#### 정적처럼 보이지만 실시간으로 움직이는 시간표
 - AI가 예측한 중간지점 시간표를 매일 생성함.
 - 당일 실시간 버스 위치, 혼잡도 데이터를 받아 조금씩 보정됨.
 - 경로 추론기는 이 갱신된 시간표를 기반으로 정적 A-Star 탐색을 실행. (속도와 정확도 모두 확보 가능)

### 개발 일정 및 단계
 - 데이터 쌓기 (~4월 말)
 - AI 모델링 시작 (학습 파이프라인 구성 및 모델 실험)
 - 경로 추론기 구현 (예측 시간표 -> 그래프 -> A-Star)
 - 안드로이드 UI 개발 (앱 구조 설계, 지도 연동, 푸시 알림 등)
 - 통합 및 최적화, 알파테스트 진행 및 디버그

### 확정된 핵심 전략들
 - 도보 이동은 cost를 높게, 버스 이동은 cost를 낮게 설정하여 A-Star 탐색
 - 휴리스틱은 AI 예측 기반 시간을 사용함. (거리는 사용하지 않는다.)
 - 예측 시간표는 움직이는 정적 구조로 설계하여 실시간성을 확보한다.
 - 실시간 이상을 감지할 때에만 AI 재보정 트리거를 넣어 연산 부하를 최소화한다.
 - 알림은 예측 출발 시각 - 도보 시간 기준으로 자동 판단한다.

### 노트
1. 상황 기반 추론 학습과 유사 패턴 통합 학습 :: 유사 상황 묶음 기반 학습
  + 비슷한 상황의 다른 성격을 지닌 데이터를 끌어와서 학습에 사용한다.
  + 보통은 특정 요일 특정 배차시간의 특정 노선이 특정 날씨에서 특정 교통혼잡도를 특정 정류장까지 오는 내내 변화하며 겪으며 도착한 시간을 추출
  + 지나치게 엄격해서 데이터 부족으로 학습 능률이 많이 떨어질 수도 있음.
  + 특정 노드 근처에서 다른 요일 다른 노선 언제든, "비슷한 혼잡도"를 보였을 때 지나가거나 움직인 시간을 끌어올 수 있다.
  + 다른 성격의 요일이거나 다른 시간대 배차 노선이지만 (노선은 똑같기 때문에 경로 일치) 실제 주행해온 교통혼잡도가 비슷하다면 학습에 사용 가능.
2. 2-Stage 예측 및 실시간 피드백 학습 시스템
  + 정적 요인 기반 예측 (전날 밤 등 버스 운행 이전 예측)
  + 실시간 교통 흐름 반영한 보정 예측 (운행 중 버스 위치, 교통혼잡도, 실시간 날씨 등)
  + 두 예측 모두 실제 도착 시간과 비교하여 loss 분석 및 학습에 반영
3. 데이터 저장은 최초 RAW파일로 .json, 가공 후 parquet (AI학습용), 학습 후 나온 결과 SQL 및 다시 parquet, 재학습으로 이어짐.
---
## 프로젝트 진행 상황
---
### 2025-04-15
#### 진행 상황 (프로젝트 리셋)
 - BIS 개선 프로젝트
 - 개선된 대중교통 길찾기 어플리케이션
#### 노트
 - 기획 점검
---
### 2025-04-16
#### 진행 상황
 - 작업용 PC와 학습용 서버를 구분하여 시스템을 구축함.
 - 개인 서버를 처음으로 만들어서 직접 headless로 구동, ssh로 접근하여 작업해봄. 신기한 경험.
#### 노트
---
### 2025-04-19
#### 진행 상황
 - 전주시 BIS API 신청 후 Service Key 수령.
 - 테스트해보았으나 관리를 안 하는지 XML 404, 001 응답 없음 (...?)
 - 전주시 BIS 홈피로 들어와서 비공식 API를 뜯어보기로 결정.
 - API 분석으로 데이터 받아보기 성공.
 - 비공식 API를 하나하나 뜯어보며 분석, 전체 노선 정보와 각 노선별 전체 정류장 정보 얻어옴.
 - 해당 정보를 기반으로 정적 데이터 정제 시작.
 - 새로운 프로젝트 디렉토리 구조 정립.
 - 버스 노선 리스트 추출
 - 노선별 부노선 리스트 추출
 - 각 노선별 정차 정류장 리스트 추출
 - 버스가 진행하는 노선별 도로상 특정 지점의 위치를 추출. (이걸로 교통흐름도 추적해서 변수로 넣을 수 있을듯?)
 - 각 노선별 출발 시간표 추출
 - 각 정류장별 통과 노선 정리
 
#### 노트
 - 비공식 API이기 때문에 우선 개인적으로 개발 이후, 배포 전 전주시에 문의를 해보도록 하자.

---
### 2025-04-20
#### 진행 상황
 - 도로교통정보 관련 API 찾음, 여기서 도로 LAT/LONG별로 ID와 GRADE(교통혼잡도)를 부여하는것으로 추정.
 - 해당 도로교통정보를 우선 ID, LAT, LONG 순으로 정렬 후 static화.
 - 기존 노선별 경로 Vertex를 도로교통정보에서 따온 ID별 Vertex와 매핑, processed에 추가.
 - 추후 모델 학습을 위해 수집해야하는 실시간 정보
 >  + A시B분에 출발한 C노선이 모든 정류장에 도착하는 실제 시간 기록 ~ Z초 간격으로 기록
 >  + 전주시 모든 도로 VERTEX에 대한 실시간 교통정보 기록 ~ Z초 간격으로 기록
 >  + 전주시 모든 도로 VERTEX에 대한 실시간 기상상태(강우(설)/기온) 기록 ~ 30분-1시간 간격으로 기록
 >  + 전주시 모든 도로 VERTEX에 대한 예측 기상정보 기록 ~ 최대 3일까지 기록
 - 공공데이터포털에서 기상청 초단기예측정보API를 신청, 매 30분마다 실시간 기상정보 수집하는 스크립트 완성.
 - 이었는데, 모든 도로 VERTEX를 기준으로 하니까 효율이 너무 떨어지지 않을까 하긴 하다. 도로별 SUB는 무시하고, 도로 하나당 1개씩 날씨를 뽑아보기로 함.
 - 사실 도로당 뽑아도 좀 의미가 있을까 싶긴 하다. 그래도 0번 SUB를 기준으로 추출 후 나머지 SUB에 대해선 0번 데이터를 복사해서 사용하도록 세팅.
 - 이었는데, 이마저도 중복 데이터가 엄청 많았다. 중복 데이터 제거를 위해 모든 도로 VERTEX(821개) 기준으로 실시간 날씨 정보를 뽑아옴.
 - 이후 "똑같은 정보를 포함하는 지점들끼리의 집합"을 생성. = 같은 기상이 보고되는 하나의 지역이라고 판단.
 - 이 집합의 대표자만 API를 통해 데이터를 받아온 뒤, 저장한다.
 - 이러면 특정 지점의 날씨를 필요로 할 때는 자신이 속해있는 집합 대표자의 정보만 가져오면 끝. 기존 821개->대략 7~8개로 저장공간 효율 1/100이 될듯.
 - 에서, 다시 날씨 정보에서 nx, ny(지역구) 기반으로 매핑. 이게 제일 정확할듯?
 - 이제 버스 실시간 위치 수집 스크립트 작성 시작.
 - 목표 : 각 요일별 매일 (평일/토요일/일요일+공휴일) 특정 시간대에 출발하는 버스를 매 30초마다 추적.
 - 30초마다 기록되는 실시간 위경도 위치와, 다음 정류장에 도착하는 시간을 하나하나 다 저장할 예정.
 - 해당 실시간 RAW 데이터는 raw/dynamicInfo/realtime_bus/{STDID}/ 아래에 YYYYmmDD_HHMM.json 형태로 저장될 예정. (YYYYmmDD_HHMM에 출발한 STDID 버스)
 - 매 30초마다 모든 노선 (분선 포함 451개)을 전부 감시하기엔 심한 리소스 낭비가 예상됨.
 - 그리고 현재 존재하는 실시간 버스 감시 API의 기능적 한계로 실제 이동중인 버스가 없을 땐 아무것도 반환하지 않음.
 - 따라서 현재 이미 정리해준 timetable을 caching이 가능한 형태로 변환.
 - 평일/토요일/일요일+공휴일별로 json파일을 따로 만들고, 특정 시간대에 출발하는 버스들을 감시 시작할 수 있도록 설정.
 - 감시가 시작되면, 포착된 버스의 번호판 정보 (PLATE_NO)를 기준으로 구분, 해당 버스의 모든 로그를 종점까지 추적.
 - STDID는 분선도 포함이므로, 중간기착지인경우도 걱정 없이 수집 가능.
 - 30초 단위 실시간 위치와, 특정 정류장에 최초로 도착하는 시간을 기록, data/raw/dynamicInfo/realtime_bus/ 아래에 STDID별로 기록 예정.
 - 10초 단위로 줄였고, 실시간 버스 추적이 일단은 되는 것으로 확인. (trackSingleBus.py)
 - 해당 파일을 스케줄러를 통해 특정 시간이 되면 특정 버스 추적을 실시하는 방향으로 자동으로 가동시킬 예정.
 - Python의 APScheduler를 사용해 "특정 요일 구분"의 미리 매핑해둔 시간대마다, 출발하는 버스들을 대상으로 10초간격 수집 계획.
#### 노트
 - 일단은... 순조롭다. 데이터만 긁어두면, 5월 초부터 모델 학습 들어가면 될 듯 하다.
 - 처음으로 그저 활용하는게 아닌 직접 모델링하는 AI라 설레는 느낌.

---
### 2025-04-21
#### 진행 상황
 - 드디어... 자동화 시작. runAll.py 제작 완료. 서버로 이식 예정.
 - 이식은 성공, 스케쥴러도 이상 없이 동작. 실제로 데이터 수집에 이상이 없는지는 아침 돼봐야 알 듯.
 - 아침, 전혀 이상한 방향으로 동작하고, 호출도 안되고 수난이다.
 - 경로 설정 등 스케줄러와 모듈 호출의 문제였지 다행히 기능적인 이상은 아니었다.
   + 기존 코드가 스케줄러에서 추적된 버스에 대한 감시를 병렬적으로 수행하지 못하였음.
   + 따라서 subprocess.popen을 이용하여 여러 스레드로 분리하여 동시에 추적/관찰 가능하도록 설정함.
 - 15시 기준 버스 로깅 등 여러가지 예외처리를 동반한 데이터 수집은 가능해졌으나, 버스의 상세 위치는 전부 NULL로 뜬다.
 - 이 NULL을 조금 해결해줘야 할 듯. 매번 긁어올 때마다 정확한 위치를 측정할 생각이다. 그래봤자 전주라 4000개다.
 - 지속적인 버그가 나와서 계속 조금씩 수정하는중. 우선은 오늘까지 테스트, 내일부터 정식으로 05시 30분 시작으로 데이터 수집.
 - 최초에 버스 감지를 못하면 이후에도 지속적으로 추적을 못한다.
 - 따라서 해당 증상을 최대한 완화시키기 위해, 출발 다다음 정류장에서 첫 감지되는 순간까지 커버할 수 있도록 조정.
 - 또한, 모두의 시계가 똑같지는 않으므로 예상보다 일찍 출발하는 경우를 감안하여 버스 출발 시간 -1분부터 해당 버스를 추적하기 시작.
 - 종점 근처에서 사라지는 버스를 종점에 찍기 위해, 종점-1번째 정류장에 도착한 버스가 갑자기 사라지면, 사라진 시점 30초가 지난 이후 종점에 도착한 것으로 간주.
 - 전주시 도로교통망에서 관리하는 노드 5500개에 대한 교통정보도 매 10초마다 수신.
 - 전주 시내버스는 완주군까지 포함되기에, 완주군쪽으로 나간 교통정보는 크게 수신하지 않기로 함. (정체가 크지 않을 것으로 예상.)
 - 만일 10초 간격으로 수집하는 버스의 위치에 전주에서 관리하는 노드 5500개 중 단 한개도 300미터 이하에 존재하지 않는다면, 평소에 막히지 않는 도로로 간주함.
 - 2시간 수집 기준 RAW데이터 대략 200MB 수집... 일당 3GB정도 예상됨.
 - 중간중간에 추적 자체가 안되는 버스들이 은근히 많이 나온다. 이거는 여기서 더 개선할 수가 없을듯. 괜히 출발지점 확장했다가 학습이 잘못된다.
 - 매일매일 100% 수집은 불가능하지만, 수집률이 0%인 노선은 없을 것. (있다면 운행을 안 하거나 BIS 미지원 노선일 가능성 농후)
 - 일단은 로그 추적만 지속적으로 하고, 데이터 수집 자체에 이상이 생기지 않는 한 방치.
 - 일주일 안에 .csv로 압축/가공하여 변환하는 툴 제작 예정. .csv를 기반으로 SQL 이식, AI 모델링 후 학습이 진행될 예정.
 - 일단은 00시 30분 이후에 스케쥴러가 자동으로 종료되는지, 05시 30분에 스케쥴러를 포함한 날씨/도로교통정보 수집도 자동으로 실행되는지 확인 필요.
 - 여기까지 진행된다면 데이터 수집은 24시간 자동으로 돌려도 이상 없음. CPU 평균 로드율 1% 미만, GPU는 미사용중이기에 모델 학습도 이상 없을 것으로 판단.
#### 노트
 - nohup python3 backend/source/scripts/runAll.py >> backend/logs/run.log 2>&1 &
   + nohup을 이용하여 runAll.py를 가동함. (백그라운드에서 계속 실행되도록 유지.) & 로그와 작동된 runall의 pid를 기록
   + 로그는 날짜별로 자동으로 분리되어 저장된다.
 - tail -f backend/logs/run.log
   + 기록되는 log를 실시간으로 관찰 가능. 디버그용.
 - ps aux | grep python | grep -E 'runAll|scheduler|traffic|weather|trackSingleBus'
   + 프로그램 종료/업데이트 시 실행중인 프로세스가 있는지 확인하고 종료하기 위한 명령어
 - pkill -f trackSingleBus.py
   + 보통 스레드가 제일 많은 모듈이 trackSingleBus이기에, 이것을 죽이면 runAll과 scheduler가 남는다.
   + runAll만 죽일 경우 subprocess로 인해 trackSingleBus는 별도의 프로세스인 탓에 작업이 끝날 때까지 죽지 않는다.
 - find . -type f -name "*.json" -print0 | xargs -0 ls -lh
   + 작업 완료로 생성된 .json 파일 열람 시도 시, 용량 확인을 통해 추적 실패로 인한 비어있는 파일을 열어볼 필요가 없게 만듦.
 - htop
   + CPU 사용량 모니터링, 실행중인 프로세스 별 CPU 사용량 등을 확인할 수 있음.
 - du -sh /{PATH} :: du -sh ~/BIS_APP/backend/data/raw/dynamicInfo
   + 특정 경로의 용량 확인 가능. 쌓이는 dynamicInfo의 용량을 체크하기 위해 사용.
 - watch -d -n 1 nvidia-smi
   + GPU 사용량 모니터링, 당장은 AI 모델링중이 아니기 때문에 사용량이 0%로 고정됨.

---
### 2025-04-22
#### 진행 상황
 - 일자별로 log를 분리해서 관찰할 수 있도록 logger.py 패치
 - 05시 30분에 자동 시작에서 오류가 남. 중복실행방지에서 cmdline이 공백일 가능성을 염두에 두지 않음.
 - 교통흐름 감지 : 기존 10초에서 1분으로 확장 (10초는 너무 간격이 짧고, 기존 내가 원했던 방향이 있어서 10초였는데 그게 의미없어져서 간격 넓힘)
 - 데이터 오염 방지를 위해, "감지중인 상황에서 같은 정류장 15분 이상 정차 시 타임아웃 처리"
 - 종점 판별 로직 업데이트 : 종점 정류장 기준 100m 접근 시 감지. (근처에서 업뎃 안되는 차량들 처리)
   + 이 경우, 실제로 종점에 들어간 경우는 별로 감지되지 않을 가능성 높음 (논리 충돌에 의해)
   + 하지만 감지되지 않은 경우의 정보가 들어가는게 더 치명적이기에 교체.

#### 노트

---
### 2025-04-23
#### 진행 상황
 - 드디어 아침에도 이상이 없었다. 안정적인 데이터 수집 환경 구축 완료.
 - 다만 좀비 프로세스가 생성이 되는데, 이유에 대한 확인이 필요해보인다.
 - 딥러닝 기초에 대해 공부중. 개념만 봤을땐 어지러웠는데, 그림으로 표현을 어떻게든 해보니까 구조가 보인다. 선형 회귀와 ReLU에 대한 이해를 얻었다.
 - 어..음..전주시 도로 위 노드를 5500개로 확장했으면서 그것을 기반으로 한 nx_ny_coords는 업데이트를 안 했다... 비상;
 - 제발 새로운게 생기지 않았길 하는 기도메타밖에 없을듯한데, 음... 데이터 3일치는 날려야 할 수도 있으려나, 모르겠다. 데이터 오염이 걱정되긴 한다.
 - 지난 기상에 대한 정보도 긁어올 수 있지 않을까?
 - 일단 이거먼저 해결하고 넘어가자. 5500개에 대한 매칭과 매핑은 금방 한다. 어렵지 않음. 이건 근데 오늘 서버 내려가면 하지.
 - 모델 계획 : 정적 모델로 1차 ETA 테이블 뽑아낸 뒤, 동적 정보 포함한 모델로 재학습.

#### 노트

---
### 2025-04-24
#### 진행 상황
 - 전처리 파이프라인 구성 후 짜는데, 생각보다 속도가 너무 느리다. 대부분의 시간은 학습에 써야하는데.
 - 병렬화를 해보기로 결심, multiprocessing.pool을 사용해보기로 생각함.
 - 병렬화 했는데..ㅠㅠ.. CPU 풀로드로 10분 가까이 작업해도 그대로임. 방향을 바꿔보기로 결심. 캐싱이 답인가. json I/O에서 병목이 추정된다.
 - 일단 30분정도 걸린다. 여기서 더 개선할 여지는 없어보임. 대략 18만개 row가 하루에 생성되기 때문에, 그냥 버텨야 할듯. HW적인 한계로 보인다.
 - 모델링을 위한 전처리 과정을 제대로 해야 학습이 제대로 될 것으로 보이기에, 데이터 정제에 심혈을 기울인다.
 - 현재 features : route_id, departure_time, day_type, stop_order, grade_1/2/3_count, PTY/RN1/T1H, target_elapsed_time으로 11개.
 - 부정확한 학습이 될 것 같아서 개선을 하기로 한다.
  + 현재까지 걸린 시간만으론 부정확한 학습이 될 것으로 추정된다.
  + 따라서, 지금 있는 raw table (2025-04-23 기준)을 기반으로 각 노선별 각 정류장에 대한 도착시간 table을 임시로 만들어둔다.
  + 해당 자료를 근거로 "내가 지금 해당 자료의 시간보다 얼마나 delay가 있는지" 파악할 수 있다.

#### 노트

---
### 2025-04-25
#### 진행 상황
 - 드디어 큰 그림이 보인다. 정리 완료.
 - AI 모델은 크게 두 종류로 나뉜다. 1차 ETA (정적 정보), 2차 ETA (동적 정보)
 - 현재 BIS 개선에 있어서, 나는 '정답이 없는 것'에 대한 추측을 하는 것이다.
 - 1차 ETA 모델은 '정답이 매일 바뀌는'것에 대한 추정이다. 즉, 정답과 함께 모델에 넣고 학습시키는게 아닌, 데이터 기반 귀납적 추론이다.
 - 지금까지의 해당 노선에서의 특정 정류장에 대한 도착시간과 요일, 날씨, 시간대 등에 대한 정보로 도착시간을 추론한다.
 - 즉, 매일 새로운 정보만 넣어주면 된다. 완성되면 알아서 계속 학습을 하게 된다.
 - 2차 ETA 모델은 실시간 버스 정보를 기반으로 보정을 하며 실시간 추정을 한다.
 - 버스가 출발하고 현위치까지의 지연시간과 해당 위치에서 현지점까지의 교통상황, 실시간 날씨를 기준으로 추론한다.
 - 추론 대상 정류장 기반으로 n정류장 이전까지만 실시간 학습/추론하도록 하면 부담을 줄이고 성능을 향상시킬 수 있다.
 - 1차 모델이 없으면 전체 시스템이 흔들리고, 2차 모델이 없으면 실시간 상황 반영이 되지 않는다.
 - 1차 모델은 통계 기반 회귀 모델로 정의하고, 2차 모델은 실시간 LSTM 등으로 정의할 수 있다.
 - 2차 모델이 이전 데이터를 "잊지 않게" 하기 위해서, 적절한 learning rate 조절이 필요할듯.
 - 우선, 1차 모델은 4월 23일 RAW 데이터를 기반으로 Baseline Table을 형성한다. 각 STDID별, 각 ORD별 도착시간을 테이블 하나로 정의해 총 4366개의 테이블을 생성한다.
 - 이후, 4월 24일 데이터를 X_y 형태로 집어넣는다. X에는 feature가, y에는 "실제 도착 시간"이 삽입된다.
 - 모델은 X와 기준점이 되는 23일 데이터를 기반으로 "이쯤에는 도착하겠지"하는 추정을 하게 된다.
 - 이후 실제 도착했던 정보와의 차이를 loss로 반영하여, 기존 저장되어있던 eta 테이블을 업데이트한다.
 - 1차 모델은 지금까지 도착했던 경향과 실시간 날씨(미래는 날씨예보)만 사용해서 학습하고 추론한다.
  + ETA 테이블 누락 시, 그냥 pass하고 추론 및 학습에서 제외된다.
  + 만약 누락된 테이블의 정보가 수집되면 그때는 생성되고 다음날부터 학습에 포함할 수 있다.
  + 누락된 STDID는 warning log로 남긴다.
 - 1차 모델에서 누락정보 관리에 대한 방법은 다음과 같다.
  + ETA 테이블에 없으면 아예 추론도 할 수 없다. (새로운 정보가 들어올 때까지 대기)
  + ETA 테이블에는 있지만 당일에 도착정보가 안 들어오면, 당일 새로운 학습은 힘들지만 다른 row들의 학습으로 인해 전날보단 좀 더 나은 추론이 가능함.
  + ETA 테이블에도 있고, 추가정보도 들어오면 완전한 학습 및 추론이 가능하다.
 - 데이터 정리하다가 실수로 교통량 raw데이터를 24일치 전부를 날려버렸다. 2차 모델은 25일치부터 가능할듯. 짜증. traffic은 24일까지 저장량 없음.
 - 23일 기준 baseline eta table 만들고, 24일 데이터 전처리할 parquet preprocess 생성 완료. 서버에 이식.
 - 이전에 데이터 전처리가 40분이 걸려서 걱정했는데, 구조 좀 뜯어고치고 뽑아내야 할 필요한 내용 숫자를 확 쳐내고 나니 17만 row가 0.57초만에 끝났다.
 - 이제 확정된 AI 파이프라인 생명주기 : (ETA테이블 누락 필링 - 1차 전처리 - 1차 학습 - ETA테이블 생성 - 2차 전처리 - 2차 학습)
 - 하루치 정보 수집이 끝나면 우선 전전날 테이블에 누락 요소가 있을 경우 전날 RAW에서 뽑아 채우는 스크립트를 먼저 만들자.
 - TODO 추가로, 사용자 요청에 따라 "n번 버스, 요일 종류, 상/하행, 배차시간"을 입력하면 ETA TABLE을 return하는 프로그램 생성 필요.
 - DEBUG용으로 STDID, 배차시간, 특정 날짜를 입력하면 해당 날짜 해당 노선의 ETA TABLE을 return하는 프로그램도 필요할듯. (학습 정도를 체크하기 위함)
 - 생명주기에서 헷갈릴 수도 있겠다. 따라서 정리.
  + 4/24 새벽(00:30)에, 4/23 정보로 baseline ETA TABLE(추론 결과로 나온거라고 가정)을 생성한다.
  + 다시 4/25 새벽(00:30)에, 4/24 정보와 4/23 ETA TABLE을 통해 학습용 데이터 전처리를 한다. 즉, 당일 쌓인 데이터와 이미 존재하던 기준점을 이용해 가공한다.
  + 이후 해당 parquet 데이터를 활용해 학습하고, 새로 추론된 4/24 ETA TABLE을 만든다.
  + 해당 TABLE과, 4/24 RAW를 이용해 2차 모델용 parquet을 가공한다.
  + 2차 모델을 학습하고, 25일 낮이 되면 실시간 추론을 실시한다.
  + 25일이 끝난 4/26 새벽(00:30)에, 다시 4/24 ETA TABLE 누락된 정보에 4/24 정보를 이용해 RAW FILLING을 한다.
    * 여기서 뒤늦게 RAW FILLING을 하는 이유는, 4/24 ETA TABLE을 만들 땐 가진 정보가 ~4/23까지만 있어야 하기 때문이다. 추론할 때에도 마찬가지다.
    * 따라서 4/24 ETA TABLE을 이용하는 4/25가 끝날 때까지, 4/23으로만 학습했다는 증거인 4/24 ETA TABLE을 망가뜨릴 수 없다.
    * 4/25가 끝나고 더 이상 4/23으로만 학습한 결과가 필요없어지면, 그 때 4/24 RAW로 4/24 ETA TABLE에 필링해준다.
  + 필링이 끝나면 필링된 4/24 ETA TABLE과 4/25 RAW를 이용해 데이터 전처리를 한다.
  + 정리 : 하루 00시 30분이 된 직후 : 해당 날짜 기준으로, 이틀 전 TABLE에 이틀 전 RAW를 이용해 필링한 후, 필링된 테이블과 하루 전 새로 모은 데이터로 전처리.
 - 2차 모델의 경우, 앞 5~10개 정류장까지만 확장해서 타겟 정류장과 현 버스 정류장을 둘다 받고, 각 정류장별 실시간 지연시간과 해당 버스 위치로부터의 현지점까지 교통흐름도 적용 가능.
 - 시계열 없이 시계열을 흉내내면서 성능도 잃지 않는 방향이다. 아마? LSTM 굳이 안써도 될듯?

#### 노트

---
### 2025-04-26
#### 진행 상황
 - 우선 유사 상황 기반 학습을 위해, 버스 STDID는 독립적인 정보를 갖고 있으므로 119번 상행 2번 버스 처럼 의미를 갖는 119A2로 매핑하여 parquet을 생성할 수 있도록 한다.
 - 그리고 드디어, 1차 ETA 모델 코드 작성 시작. pytorch 기반으로, GPU가속을 이용해 학습시킬 예정이다.
   + (그럴 일은 없겠지만) 날씨가 NULL이 입력되어있다면 결측치 처리로 0으로 바꿔준 뒤 진행한다.
   + 모델은 MLP 모델을 선택했다. 64차원으로 증가 후 자체적으로 학습, 1차원으로 회귀하는 모델로 만들었다.
   + 학습률은 0.001로 설정하고, 300회의 epoch를 통해 학습하도록 설정했다.
   + 매 10회 epoch마다 loss를 확인할 수 있도록 했다.
 - 생각보다 결과가 처참하다. 데이터 전처리부터 다시 해야할듯. (모델이 아예 근접한 추론도 못한다.)
 - 시간을 sin/cos 인코딩을 통해 하루 주기 순환이라고 모델에게 이해시켜준다. 그냥 0분~1440분으로 적으면 순환하는 구조로 이해하기 어려울 수 있다.
 - 모델에는 큰 문제 자체는 없었다. 단지 "출발시간 기준"으로 delay를 연산하고 있어서 생긴 사소한 문제였던걸로 추정된다.
 - feature로 정류장번호 추가했는데 갑자기 모델이 다 망가졌다. 진짜 멘탈 와르르. 왜지?
 - 고쳐내긴 했는데, 모델이 너무 멍청하다. 좀 고쳐봐야할듯. 데이터 부족인지, 모델 문제인진 모르겠는데 아무리 봐도 모델 문제다.

---
### 2025-04-27
#### 진행 상황
 - 모델이 멍청해서 다시 갈아엎어보기로 했다. 데이터 전처리부터, 다시 논리를 명확하게 해서.
 - 지금 약간 이도저도 아닌 상황이 되어버렸는데, 데이터가 부족해서 여러 데이터를 한번에 쓰자니 기준점이 없고, 기준점을 잡자니 데이터에 왜곡이 생긴다.
 - 일단은 fine-tuning쪽으로 잡고, 한 일주일정도 학습하면서 추이를 지켜보기로 했다.
 - 이어서 쌓으려고 해도, 현재 MLP로서는 feature에서 시간을 추론하는, 주객이 전도된 모습을 보인다.
 - 방향성을 확실히 잡고 간다. 지금 나는 추론할 수 없는 것을 통계적 기반으로 그나마 유사하게 추론하기 위한 모델을 만드는중이다.
 - 산술평균이 아닌 AI를 가져온 이유도, 경향성에서 어느정도 힌트를 얻을 수 있지 않을까 하는 의미였다.
 - 따라서, 실 데이터를 가장 무겁게, 그리고 요일과 출발시간 정보 또한 무겁게 처리한다. 임베딩을 통해서 무게가 낮은 정보는 차원을 늘려 무게감을 희석시킨다.
 - 정류장별 도착 시간을 6개로 나눠, 모델이 각 시간대별로의 경향성을 파악할 수 있도록 돕는다.
   + 이른 아침/출근시간/점심시간/오후/퇴근시간/야간 6단계로 나눴다.
 - 다시, 갈아엎는다. parquet 생성 - 학습 - 추론까지.
 - 드디어 Loss가 6만 밑으로 떨어졌다. 처음인듯. 학습 자체에 시간이 이렇게 오래걸리는것도 처음이다. 뭔가 잘 될 것 같은 느낌.
 - 드디어 어느정도 학습 데이터를 믿을 수 있는 수준까지 왔다!!! 임베딩이 정답이었다.
 - 모델 1이 거의 완성됐다. batch = 2048, epoch = 600, lr = 0.0005 수준으로 대략 10분정도 소요된다. (1회 학습당)
 - 현재 "기존 값에 중점을 둘지, 새로운 값에 중점을 둘지, 적절히 섞을지"에 대한 판단을 위해, 일주일정도 실험을 해야한다.
   + 1번 실험 : 기존 안정적인 feature만 사용한다. (기존 값에 더 치우친 결과를 낸다. 데이터가 무수히 많을땐 안정적일지도.)
   + 2번 실험 : 하루는 기존 버전으로, 하루는 새로운 데이터에 가중을 두는 학습을 한다.
   + 3번 실험 : 매일매일 새로 들어오는 데이터에 힘을 조금 실어준다. (즉, (이틀치 평균 + 3일)의 평균과, 그것과 4일차의 평균..이 누적)
   + 일주일간 실험 이후 가장 유사한 퍼포먼스를 내는 모델로 확정하면, 1차 ETA 모델링은 종료된다. 이후 매일매일 자동학습이 가능해진다.
 - 바보같이 일기예보는 수집을 안하고 있었다. 28일 기준 일기예보부터 수집될 예정. runAll.py에 이식 완료.

---
### 2025-04-29
#### 진행 상황
 - 시험공부로 인해 진행상황이 더디다. 어차피 7일치 데이터로 실험하기로 했으니 속도를 많이 낮추고 다른쪽에 집중하는중.
 - 한가지 최적화 제안; multiprocessing.Pool은 Process기반 병렬화인데, Parquet 전처리기의 경우 결국 하나의 프로세스가 같은 결과를 만들기 위해 작업하는것.
 - 이 경우 프로세스 기반 병렬화보단 스레드 기반 병렬화가 훨씬 효율적이지 않은지? (프로세스 컨텍스트 스위칭 오버헤드 감소를 위해~)
 - concurrent.future.ThreadPoolExecutor의 사용 가능성에 대해 한번 검토해보자. (사실 현행으로 100% 풀로드 3초 이내니까 안정적이긴 하지만, 최적화.)

---
### 2025-05-01
#### 진행 상황
 - 이제야 조금 진행할 수 있을듯 싶다.
 - 아까 오후 1시쯤, 폭우가 쏟아지던 시점에 실시간 날씨가 하나도 안 받아졌다. 이거 처리 잘 해줘야할듯.
 - 일단 지금 당장 null을 가장 가까운 시간대로 fill해주는 tool 돌리고, AI학습루틴에 00시 30분이 되면 제일 먼저 날씨 null부터 채우고 가야할듯.

---
### 2025-05-02
#### 진행 상황
 - dim 6/7을 번갈아가며 실험해보자 한 건 내 AI지식의 부족함 때문에 나올 수 있었던 idea였다.
 - 일단은 2번 버전은 제외하고, 1,3번 (각각 dim 6/7버전)으로 실험을 한 뒤, 만족스럽지 않은 결과가 나온다면 그 때 masking을 통해 실험 예정.
 - 현재까지 대략 일주일 이상의 자료가 모였으니, 우선 학습을 반복하며 자료를 만들어보자.
 - dim 6: 기존 데이터에 집중하는 방식, dim 7: 새로운 데이터에 가중치를 조금 더 주는 방식
 - 5/1기준까지 학습 진행 후, 추론된 5/1 데이터와 5/1 RAW를 상호 비교하여 성능지표 체크 예정. 더 우세한 쪽으로 당장 오늘밤부터 자동화 돌릴 예정.

### 비상 (망함)
 - ls한다는걸 아무생각없이 rm -r data를 해버렸다.
 - 4/23이후 모든 데이터를 손실했다. 이제 연휴기간이라 평일 데이터는 모을 수도 없는데 말이지.
 - 10일치를 날렸다. 믿을 수가 없다.
 - 5월 둘째주부터 당장 2차 ETA랑 길찾기 에이전트 만들어야하는데. 아..
 - 우선 9일까지는 보류다. 3 4 5 6 7 8 9일 해서 일주일치 모으고 다시 봐야겠다.

---
### 2025-05-03
#### 진행 상황
 - 부러진 멘탈을 휘어잡고, 다시 시작. 어차피 1차 모델은 시간이 약이니, 그 사이에 2차 모델링을 한다.
 - 기존에 수집했었던 data/raw/staticInfo/vtx의 정체를 드디어 알았다. 그냥 "지도상에 버스 경로를 보이기 위한" 노드였다.
 - 정류장이나 교차로 인근에 node가 몰려있고, 일직선으로 그리는 경우에는 전혀 해당사항이 없다.
 - 따라서 데이터를 하나 새로 만들어야한다. 일단 vtx 안에 stdid별로 있는 정보는, 지도를 그리기 위한 선이니까 노선을 정확히 따라가긴 한다.
 - 즉, 해당 vtx들을 직선으로 긋고, 그 사이에 있는 가장 가까운 traffic node를 2-3개정도 순차적으로 잡으면 된다. (traffic node는 매분 data로 들어온다.)
 - 만약 A vtx node와 B vtx node가 있다고 치자. 이들은 각각의 LAT/LNG값을 갖고 있다. 이것을 직선으로 잇는다.
 - 해당 직선에서 A -> B로 가는 방향으로 가장 가까운 traffic node를 3개정도 찾아서 순차적으로 저장한다. 만약 사이에 정류장을 지나친다면 반드시 추가한다. (정류장 정보는 이미 있다.)
 - 여기서, A/B도 포함한다. 즉 A/B에 해당하는 traffic node도 순차적으로 포함한다.
 - 이러면 만약 기존 vtx node가 341개가 있었다면, ×4한 수치인 대략 1400개정도의 새로운 node집합이 형성될 것이다.
 - 여기서, A/B는 정류장일 수도 있고, 정류장을 지나칠 수도 있다. (눈으로 확인한 결과 직선상에 정류장이 있다면 그냥 무시한다.)
 - 새로운 node 집합은, 시점부터 종점까지 운행중인 모든 선을 지나가는 교통 노드들을 순차적으로 보여줄 것이다.
 - 이제 다시 한 번 전처리를 한다. 새로운 노드 집합은 기존 vtx node를 기준으로 잡혔기 때문에, 매우 짧은 거리 안에 노드가 몰려있는것들이 있을 것이다.
   + 이를테면, 노선이 ㄷ자 형태를 띠고 윗부분에 정류장 하나, 아랫부분에 다음 정류장이 위치한다고 가정하자.
   + 이 경우 ㄷ의 왼쪽변 꺾이는 부분에 vtx node는 상당수 몰려있는 형태가 된다. 노선이 꺾이는 구간이니까. (정류장이 아닌데도 불구하고 말이다.)
   + 이런걸 처리해주는것이다. 정류장과 정류장 사이 3개정도의 노드만 남길 수 있도록.
 - 최종적으로 처리가 된다면, 각 노선별 진행방향 상에서 정류장-정류장 사이 노드가 3개정도씩 있는 결과값이 나올 수 있다.
 - 정류장이 60개라면, 새로이 생성된 노드는 대략 240개정도일 것. (경로 따라서, 중간지점들이 순차적으로 sort된 형태로.)
 - 이런 정보를 모든 STDID에 대해 정리하는 작업을 진행해야 한다.
 - 여기까지 끝나면, 버스의 10초 단위 로그에 "현재 새로이 생성된 노드 중, 자신의 node에서 어디를 지나고 있는지"를 판별하는 형태로 바꿔야 한다.
 - 즉, 데이터 손실을 줄이기 위해선 time-limit이 걸려있다. 5월 4일 5시 30분 안에 모든 작업을 끝마치고 서버에 이식해야 한다.

---
### 2025-05-04
#### 진행 상황
 - 엄청난 상황에 직면했다.
 - 나는 전주시 제공 데이터가 무결하다고 생각했는데, 무결하지가 않았다. 버스 경로를 추적하는 VTX가 시외부분에선 정말 대강 대충 그려둔 탓에 정류장을 따라가질 않는다.
 - 101개 노선에 문제를 발견했고, 원본 데이터 자체에 생긴 문제이기 때문에 이것만 수작업을 해주기로 했다. 그렇게 많은 부분을 고칠 필요 없고 노선 그림만 잘 그려지게 만들어주면 된다.
 - 1시간 이내에 끝낼 수 있을 것으로 예상.
 - 3시간 걸렸다. 상습 구간이 있었다. 이서회차지 부근, 월드컵경기장 부군, 완주IC 진출로 부근, 소양면 인근, 송천동 종점, 평화동 인근 정류장 1개에 대해서 난 오류가 대부분이었다.
 - 최종적으로, 끝났다. 완전히 새로운 경로 노드를 완성했다.
 - 새로운 경로 노드는 "실제 버스 운행 경로"를 따라, 200m 단위로 새로운 노드를 하나씩 찍으며, 중간중간에 정류장을 만나면 바로 정류장을 찍는다.
 - 새로 찍은 이 노드는 전주시 451개 STDID를 전부 커버한다.
 - 괜히 머리쓰다가 다 뒤집어질 가능성도 있고, 데이터가 부정확한거면 어쩔 수 없이 수작업이 들어가야한다고 판단했다.
 - 다음 순서는 아래와 같이 진행된다:
   1. 먼저 trackSingleBus를 기존 교통 노드 탐색에서 경로 노드 탐색 로직으로 바꾼다. 반경은, 200미터로.
    + 만약 경로 노드를 감지하지 못했다면, 가장 최근 노드를 그대로 가져오도록 설정한다. 사실 경로를 따라 경로 노드를 만들어놨기 때문에 이럴 일은 없겠지만, 혹시 모를 안전장치를 추가한다.
    + 경로 노드에는 인덱스가 있다. 거리 기반으로만 판단해서 가장 가까운걸 가져오면 인덱스를 무시할 가능성이 매우 높아진다.
    + 따라서, 버스는 ORD를 기준으로 진행하므로 STOP ORD와 연계된 흐름이 필요하다. 만약 현 버스가 ORD1을 막 지난 상황이라면 ORD 1~2 사이에 있는 경로 노드만 잡을 수 있도록 해야 한다.
    + 한 가지 문제 추가. 경로 노드 파일에는 STOP이 저장되어있지만 몇 번 노드가 몇 번 STOP_ORD인지는 모른다. 이걸 미리 매핑해야하나...
    + 매핑했다. ORD 1~2 사이, 등등... 이걸 기반으로 판단하면 된다. 점점 더 정확해지는듯?
   2. 저렇게 바꾸고 서버에 이식한 뒤 정상작동하는걸 먼저 확인한다. (realtime_pos에 저장.)
   3. 확인이 끝나면, 각각의 루트 노드를 반경 100미터 기준으로 인근 교통 노드를 매핑할 수 있도록 한다. 만약 매핑이 안 된다면, 인근에 매핑된걸 전파받는다. 단, "직접 매핑된 정보만" 전파된다.
    + 즉, 전파된 노드의 정보는 받지 않는다.
   4. 매핑까지 끝나면, 매 분 교통흐름을 받아올 떄마다 바로바로 route_nodes_cong/{YYYYmmDD_HHMM}.json 파일에 각 STDID별 모든 route nodes에 대해 각각의 혼잡도를 찾아서 저장한다.
 - 일단 trackSingleBus 수정해서 서버 이식했다. 이게 제대로만 나오면, 나머지는 새벽 내내 진행해도 전혀 촉박하지 않게 할 수 있다.
 - 일단... 4까지 끝. 5/5 데이터부터 사실상 이롭게 이용할 수 있지 않을까. 힘드네. 사실 데이터 안 날려먹었어도 5/5부터 쓸 수 있었을까 싶다. 1차 모델링이라도 과거 데이터부터 써먹을 수 있었을까?
 - 기능 정상동작 확인 완료, 서버 이식. 급한 불은 껐다. 이제 2차 모델링에 활용할 교통 데이터가 좋은 흐름으로 생성될 예정. 인식된 교통 노드가 없으면 혼잡도를 1로 간주한다.

---
### 2025-05-05
#### 진행 상황
 - 1차 모델 추론 과정에서 심각한 오류를 찾아 바로 수정했다.
 - 기존 : 추론에 있어서 날씨를 예보가 아닌 관측날씨만 가져오고 있었다. 요일 또한 하루 전 데이터로 가져와, 사실상 의미 없는 이상한 예측을 수행중이었다.
 - 변경 : 예보 날씨 제대로 적용, 요일도 target date에 맞는 요일을 사용하도록 조정함. 이 과정에서 X dense 수정이 필요하여 새로 구성 후 추론하도록 하였다.
 - 추가로, 1차 모델에서 기존 비교군으로 삼던 X dense 내부에 있던 actual elapsed time이 사실상 정답을 알려주는 정답 누수의 요인이었다. 따라서 비교군 실험은 필요가 없어졌다.
 - 데이터를 3일치로 확장할 방법을 찾은 줄 알았으나, 과적합 위험으로 인해 버리기로 했다. (3일치 예측으로 3일치 모두 학습하는~ 대충 이런 내용임.)
 - 로그 관련해서 문제가 조금 있어 수정했다.
 - 현재 1차 모델에서 날씨가 갖는 무게감을 아직 잘 모르겠다. 실험을 통해, 무게감을 조금 낮추거나 높이거나 하는 식의로 조정이 필요해보인다.
 - 임베딩을 요일X시간대로 합쳐서 해보는것도 괜찮을듯. 나중에 실험해보자. 모델이 판단하게 맡기지 말고, 애초부터 합쳐서 넘겨주는것.
 - 실험을 위한 2코드 4종을 만들었다. 데이터 쌓고 바로 실험해보면 될듯.

---
### 2025-05-13
#### 진행 상황
 - 중간고사 전부 끝나고 일주일정도 쉬었다. (그간 데이터는 착실히 쌓였다.)
 - 쉬다 와서 쌓인 데이터로 학습-추론해봤는데, 과거 데이터 날리기 이전보다 뭔가 많이 망가진 느낌이 난다.
 - 일단, 당장으로서는 전혀 모델이 제대로 된 학습과 추론을 하고 있다는 판단이 서질 않는다.
 - 우선 단기적으로 1차 모델이라도 확실히 잡고 앞으로 나갈 수 있도록, 좀 열과 성의를 다해야겠다. 지금으로썬 아무것도 할 수 있는게 없다. 완성품도 미지수.
 - 따라서 1차 모델을 완전히 재점검한다. 당장으로선 추론이 아닌 그냥 숫자 마구잡이로 뱉기가 된다.
 - 우선 1차 모델을 다시 짜기 위해, 현재 상황을 검토.
   + 현재 각 feature의 사용 목적
   > route_id: 노선별 흐름/패턴 구분. 본선/분선이 섞이는 상황 고려
   > node_id: 정류장 위치가 미치는 고유한 영향 파악
   > weekday: 요일별 패턴 인식 (평일, 토요일, 일요일 및 공휴일)
   > departure_time_sin/cos: 시간 흐름 인식 (연속적 시계열로서의 시간: sin/cos를 통해 하루의 반복을 알게 해준다.)
   > departure_time_group: 의미 있는 시간대 구간을 구분하기 위해 사용
   > PTY, RN1, T1H: 날씨 영향 (강수형태, 강수량, 기온) :: 보조적 역할
   + 추가할 수 있는 feature
   > ORD_ratio: 노선 상 상대 위치 판단 (전체 노선 흐름 중 어느 부분에 있는지)
   > mean_elapsed_at_stop: 정류장 단위의 통계 기반 baseline

   + 너무 고차원의 임베딩은 학습이 불안정하고 과적합 위험이 있을 수 있다.
   + 반면 너무 저차원의 임베딩은 표현력이 떨어진다. 적당히 높아야 모델이 복잡한 관계를 학습할 수 있다.
   + 즉, 차원은 모델이 feature의 복잡한 관계를 학습할 수 있도록 만드는 요소.

   + Label Encoding: 정수 인코딩~ 자연어를 정수로 인코딩한다. 하지만 이대로 사용 시 모델은 정수의 크기로 순서 정보를 얻어 왜곡 발생 가능성이 생긴다.
   + Embedding: 따라서 정수ID를 의미있는 실수 벡터로 매핑한다. 비슷한 ID끼리 비슷한 벡터를 학습한다.

   + 현행 departure_time_group은 유지한다. sin/cos는 연속 시간 흐름에 적합하지만, group은 명시적인 시간대에 의미를 부여한다.
   + time group은 embedding으로 처리해서 같은 그룹끼리 묶인다는 의미를 확실히 알 수 있게 처리하도록 하자.

   + ORD_ratio는 유의미하게 쓸 수 있도록 만든다. 모델은 멍청하다. 단순히 실수값을 주면 전/중/후반부라는 사실을 당연히 인지하지 못한다.
   + 따라서 route_id embedding에서 context를 받아 ORD_ratio에서 위치 힌트를 제공하는것에 낫다.
   + 위치 힌트는 정규화+범주화 둘 다 같이 주는게 당장으로써는 제일 안전하다고 본다.

   + mean_elapsed_at_stop은 매일 누적 합 느낌으로 계산하면, 훨씬 적은 비용으로 계산하고, 학습 및 추론에 참조할 산술평균값을 얻어낼 수 있다.
   + stop_sequence_length또한 모델이 인식할 수 없기 때문에, ORD_ratio와 묶어서 같이 예산해서 사용하는 방법을 생각해본다.
   + 혹은 stop_squence_length를 bucket화해서 embedding처리하는 방안도 생각해본다.

---
### 2025-05-14
#### 진행 상황
 - 모델을 완전히 갈아엎으며, X에 들어가는 feature들을 정확하게 재정의한다.
 - 기본적으로 모델은 "멍청하다". 내가 각 feature의 의미를 알려주는게 아닌 이상 절대로 알아차리지 못한다.
 - 따라서 모델에게 각 feature의 의미를 정말 하나하나 다 알려줘야 하며, 의미가 엮이는 것이 있다면 concat해서 처리해야한다.

 - 우선 현재까지 뽑은 X feature들이다.
  > route_id: 노선 고유 ID
  > node_id: 정류장 고유 ID
  > weekday: 평일/토요일/일요일 + 공휴일
  > departure time sin/cos: 버스 출발 시각의 주기적 표현
  > departure time group: 시간대 그룹
  > PTY, RN1, T1H: 날씨 예보 (학습은 관측)
  > ORD_ratio: 노선 내 현재 상대적 위치
  > ORD: 절대 순서 정류장 번호
  > mean elapsed at stop: 해당 정류장 평균 소요시간 (각 stdid별, raw기반)
  > LAT/LNG: 정류장 좌표(위/경도)
 - feature engineering이 최우선이다. 당장은 X, y, loss를 제대로 잡는데 집중해야한다.
 - x feature 잡고 나면, 일단 데이터가 정상인지. outlier는 없는지먼저 한번 검사해보자. 당장은 신뢰하고있지만 빗나가는 경우도 생각해야한다.

 - 1차 ETA 모델 최종 Feature Set 정리
  + [Route Related] route_id -> bus_number, direction, branch_num으로 분리.
   * 기존 route_id는 라벨링-임베딩을 적용했는데, 이 경우 STDID를 라벨링-임베딩하는거랑 똑같은 맥락이 된다. (모든 노선 본/분선이 독립적)
   * 그냥 의미없는짓이었음. 따라서 3개로 분리해서 각각에게 의미부여를 한다.
   * 하지만, 각각을 독립적으로 임베딩할 경우 전혀 관련없는것들 사이에 벡터가 생길 수도 있다.
   * 예시로, 110번 상행 본선과 5001번 하행 본선이 branch_num이 1로 같다는 이유로 비슷한 벡터가 생길 가능성이 있다.
   * 따라서 bus_number > direction > branch_num 순으로 위계적 종속 구조를 만들어 모델에 명시적으로 반영한다.
   * direction, branch_num은 각각 상위 요소를 조건으로 한 조건부 임베딩 구조로 설계한다.
   * 이를 통해 같은 노선 내 상/하행끼리는 약한 연관성, 같은 방향은 강한 연관성을 띠며 다른 노선과는 완전한 분리를 할 수 있다.
   * 결론: categorical -> embedding -> MLP 가공 후 사용.
   * dims: bus number = 8, direction = 4, branch num = 4
  + [Order Related] ORD, ORD_ratio, mean_elapsed
   * 기존 모델은 현재 정류장의 순서를 이해하지 못했다. 따라서 ratio를 추가해서, "진행률 개념"을 습득할 수 있게 한다.
   * 하지만 단순 float를 넣는다고 모델이 이걸 진행률이라고 인식할 수 없다. 멍청하다.
   * 따라서 mini MLP로 가공하여, progress 개념을 확실히 명시해준다.
   * ORD는 상위개념으로 확장하지 않고, STDID 개념에 종속시킨다. 이래도 route_id를 분리한 상위개념에서 비슷한 벡터를 형성한다.
   * 그리고 추론에 있어서 도움을 주기 위해, 해당 ORD에 대한 현재까지 평균 도착시간을 입력한다.
   * 해당 개념은 전체, 요일별, 요일+시간대별로 3중으로 입력한다.
   * 위 3대 개념을 MLP화한다.
   * dims: ord = 8, mean = 16 // 의미는 같지만 기능이 달라서 분리해야한다.
  + [Location Related] node_id, mean_interval
   * 특정 정류장을 기준으로 근처에 버스가 오래 걸리는 구간을 feature로 넣어줄 수 있다.
   * 따라서 mean interval 도입, 얘도 3종류로 나눈다.
   * node id는 임베딩, 그리고 mean interval은 3종 mlp를 한 뒤, 그 결과와 임베딩된 node id를 다시 mlp한다.
   * dims: node_id = 16, mean = 8, node_context = 24 -> 16
  + [Time Related] weekday, departure_timegroup, weekday_timegroup, departure_time_sin/cos
   * weekday, timegroup, weekday_timegroup 3종류로 나눈다. 요일 / 시간대그룹별 / 요일+시간대그룹별로 나누고자 하는 목적.
   * 시간의 연속성을 부여하기 위해 sin/cos로 바꾼 departure time도 넣는다.
   * 전부 임베딩, 각 4 / 8 / 12 / 2.
   * dims: concat = 26 -> 16
   * time group을 강조하고싶다면 따로 분리해도 된다.
  + [Weather Related] PTY, RN1, T1H
   * PTY dims: 4, concat = 6 -> 8
  + [Self Predicted] prev_pred_elapsed
   * 전날 자신이 추론한 결과, 즉 ETA TABLE에서 값을 그대로 가져온다. 이건 학습용도이고, 추론시에는 사용하지 않는 feature다.
   * embedding mlp dims 1 -> 4
#### 노트
 - embedding dim = min(50, int(category^0.25))

---
### 2025-05-15
#### 진행 상황
 - 1차 모델 최종 구성
    1. 최초 학습 시작 구간
     + 5월 5일
     > mean_elapsed_*, mean_interval_* 관련한 평균 28종 생성을 위해 데이터 사용
     > 해당 날짜에 대해선 별도의 학습/추론 없을 예정
     + 5월 6일
     > 최초 학습 시작. (replay only)
     > self-correction 생략
     > mean은 5월 5일 기준으로만 사용
     > 요일이 weekday, saturday인 timegroup 0~7의 임의의 데이터 1개씩을 미래에서 추출해서 사용.
     + 5월 7일 이후~
     > full loop 시작.
     >  > 전날 추론한 ETA Table과 실제 운행을 기록한 raw간에 차이로 self-correction 학습 수행
     >  > 전날 실제 운행기록만으로 replay 학습 수행
     >  > 학습이 종료된 후, 전날까지 데이터만으로 mean을 재계산
     >  > 재계산된 mean과 미래 정보를 바탕으로 당일~ ETA Table을 추론
    
    2. X features 구조
     + Route related
     > bus number, direction, branch num으로 구성
     > 각 요소들에 대해 categorical 분류 후 embedding, MLP 적용.
     > 요소들에 대해선 계층적 종속성을 반영. bus number > direction > branch num으로 종속된다는걸 모델에 알려줘야 함.
     > dims: 8 + 4 + 4로 16차원 구성
     + Order related
     > ORD_ratio, mean_elapsed로 구성
     > ORD_ratio 단독 mini MLP 구성
     > mean_elapsed_* 3종에 대해서 계층적 종속성을 반영한 MLP 적용. (total > weekday > weekday_timegroup)
     > dims: ord_ratio = 8, mean_elapsed = 16
     + Location
     > node_id, mean_interval로 구성
     > mean_interval_* 3종에 대해서 계층적 종속성을 반영한 MLP 적용
     > 이후 node_id를 임베딩한 결과와 다시 fused MLP를 적용
     > dims: node_id = 16, mean_interval = 8, concat 후 MLP로 16차원 구성성
     + Time
     > weekday, timegroup, weekday timegroup, departure time sin/cos로 구성
     > 각각에 대해 embedding 적용, 각 4 / 8 / 12 / 2 dims로 구성
     > concat 후 MLP: 16차원 구성
     + Weather
     > PTY, RN1, T1H로 구성
     > PTY는 embedding (dims: 4), 나머지 둘은 float로 구성
     > dims: concat 후 MLP 적용 = 8
     + Self Info
     > prev_pred_elapsed
     > self-correction시에만 사용. 즉, replay 학습과 추론시에는 사용하지 않는다.
     - 각각의 모든 경우에 대해 0~1 scale로 정규화를 진행할 예정.
    
    3. Target y 및 Loss 구성
     + y = real_elapsed
     > 마찬가지로, y 또한 min-max 정규화를 통해 출발부터 현재 정류장까지의 누적 시간을 표현할 예정
     + Loss: heteroscedastic regression
     > loss = ((y - pred_mean)**2 * exp(-pred_log_var) + pred_log_var).mean()
     > ETA 예측과 동시에 불확실성(신뢰구간)에 대한 정보를 내포함
     + 추가 :: 순서 제약 Loss
     > 같은 trip 내 ORD_i < ORD_j인 경우에, pred_i < pred_j가 되도록, ReLU(pred_i - pred_j) 기반 penalty 추가
    
    4. Train Parameters
     + Batch Size: 512
     + Epoch: 15
     + Learning Rate: 0.001
     + Optimizer: Adam
    
    5. 기타 방어장치
     + mean fallback: wd_tg 없으면 -> weekday 없으면 -> total 사용 순
     + ETA 순서 역행 방지: 순서 제약 Loss로 해결, 추론 후 검증과 postprocess로 적용
     + self-correction 안정화: ETA 생성 당시의 feature를 사용하여 모델이 스스로 잘못된 추론을 했다는 점을 인지하도록 훈련.
  
 - 위 정리를 기반으로 1차 모델을 만들기 시작.
 - 우선 mean 28종을 2개 각각 만드는 코드를 짜야 한다.
  + 아이디어: 각 stdid_ord별 값을 추출하는데, 해당 필드를 mean, stop_id와 num으로 구성한다. stdid_ord에는 앞 3가지와 별도로 weekday, timegroup 정보도 포함해야 한다.
   * stop_id는 mean_interval_* 활용 시 사용 가능.
   * num은 이후에 mean 재계산 시 중복 계산을 막기 위한 장치. 즉, 현재 저장된 mean에 num을 곱하여 sum으로 만든 뒤, 해당 값에 새로운 raw를 더하고 num도 1 증가. 이후 num으로 다시 나눠주면 mean이 된다.
   * 즉, 우리는 그냥 각 ORD에 대한 mean을 구하는게 가장 힘들다. 이게 I/O를 가장 많이 일으키는 구간이기 때문.
   * 따라서 각 ORD에 대한 mean만 구하면, 나머지는 금방 수행할 수 있다.
   * 각 ORD에 대한 mean을 구한 뒤, weekday_timegroup별로 합쳐서 mean을 구한다. 여기서부터는 별도의 파일로 분리한다.
   * 계산 방법은 최초 mean을 구하는 방식과 똑같이 한다. 각 mean에 num을 곱하여 sum으로 만든 뒤, 필요한 모든 sum을 더하고 같이 num도 더한다. 그 이후 num으로 나눠주면 새로운 mean이 된다.
   * weekday_timegroup에 대한 mean을 구했다면, 다시 weekday에 대한 mean도 똑같은 방법으로 구한다.
   * 마찬가지로 total도 구할 수 있다. 여기까지 하면 mean_elapsed_time 28종을 완성할 수 있다.
   * 다시, mean_interval을 구해야한다. 여기는 살짝 다른 방법으로 구현해야한다.
   * 최초 ORD별 mean을 구해둔 파일과, 모든 stop_id가 있는 파일을 같이 열어둔다. 이후 stop_id를 하나씩 순회한다.
   * 모든 stop_id에 대해, 각각의 ord별 mean이 저장된곳을 순회한다. (즉, O(n^2)) 여기서 자신과 같은 stop_id를 가진 ord가 있다면, 해당 mean과 이전 ord의 mean간의 차이를 가져온다.
   * 만약 이 n^2를 고칠 수 있는 최적의 방법이 있으면 아주 아주 좋다!!
   * 이 때, num도 같이 가져오며 새로운 정보를 가져올 때마다의 mean 계산은 위의 방법과 동일하게 수행한다.
   * 어...근데 생각해보다보니, 이러면 각 정류장들에 대해 28종 interval이 생기게 되는데? 연산량이 너무 많아지는건 아닐까? 갑자기 걱정이 된다.
   * 정류장이 2573개. 각각에 대해 28종 interval이 생성.. 조금 어지러운데?
   * 보다 보니, n^2을 획기적으로 해결할 수 있는 방법을 찾았다. 기존 갖고 있던 파일중에 stop_to_routes라는 폴더가 있었다.
   * 안에는 각 정류장별로 지나가는 특정 노선의 정보를 담게 만든 파일이다. 단, 안에는 stdid밖에 없다. 따라서 이 정보를 조금 수정할 필요가 있어보인다. 내부에 route_id, ord를 포함시키면 즉시 접근이 가능하다고 본다.
   * 해당 기능을 수행하는 코드는 buildStopIndex.py이다. 이걸 조금 수정하면 내가 원하는 결과를 얻을 수 있을 것. (각 정류장ID별 json 파일 내부에 : 지나가는 버스에 대한 정보를 담고 있음.)
   * 하지만 2573개 정류장 각각에 대해 28종 interval을 생성하는 문제는 한번 생각해볼 필요가 있어보인다. 크게 상관이 없나 싶긴 하지만...
   * 암튼 하게 된다면, 모든 정류장 각각에 대해 weekday_timegroup별 mean을 만들 수 있다. 이걸 다시 weekday별로, 다시 total로 만들 수 있다. 위와 똑같은 논리로.
 - 여기까지 된다면, 1차 학습을 위한 모델을 짜야 한다. 이와 같이 전처리 코드도 짜야 한다.
 - 이후 추론을 하게 되는데, 음... 약간 이 과정에서 기존에 하던 postprocess에 관한 생각을 조금 해봐야 할듯 싶다. 기존과 방향이 완전히 달라진 탓에, 기존 방법을 유지할 수 없을 것으로 보인다.

---
### 2025-05-16
#### 진행 상황
 - mean_interval, mean_elapsed 완성.
 - 해당 두 파일은 날짜별로 생성되며, elapsed는 stdid별 ord에 대한 28종 mean을, interval은 stop_id당 28종 mean을 구한다.
 - 해당 파일을 만들기 위해 기존 buildStopIndex.py 파일을 수정했다. (stop id당 stdid, ord를 포함하도록)
 - 경우에 따라 만약 모델이 답도 없을 정도로 추론을 못 한다면... 진짜 그냥 mean값으로 1차 모델로서 활용해야할지도 모른다.
 - mean_interval, mean_elapsed를 36종으로 확장했다. 기존 중위계층에 weekday만 있었던걸 timegroup을 독립적으로 추가해줬다.

---
### 2025-05-19
#### 진행 상황
 - 주말간에 1차 모델 설계 및 데이터 전처리를 시도했는데, 하면 할 수록 모델에게 내 의도를 제대로 전달하지 않고 쉽게 가려고 했던 점들이 자꾸 나타난다.
 - 이번주 금요일까진 어떻게든 1차 모델을 완성해야한다. 그래야 진행이 된다.
 - 생각보다 데이터 전처리/학습 과정이 1차 모델임에도 굉장히 방대해서, 조금 제대로 된 정리가 필요해보인다.
 - 일단 1차 모델 틀 완성. 전처리기부터 만들려니 도저히 안될거같아서 모델 정의 해놓고, 학습세트 정의시킨 뒤에 전처리기 만들어야겠다.

---
### 2025-05-20
#### 진행 상황
 - 1차 모델을 돌리기 위해 데이터 전처리기를 만들었는데 계속 버그가 난다. 미치겠다.
 - 버그 원인 규명에만 몇 시간째 쏟아붓는중. 뭔가 이러다가 아무것도 못 하고 끝날 것만 같아서 무섭다.
 - 버그는 내 머리에 난 거였다. 첫날 평균만 넣었는데 당연히 요일=전체지.
 - 우여곡절 끝에 첫 모델 학습을 했는데, 처음 써보는 방식의 loss를 사용해봐서 그런지 몰라도 loss가 음수가 나온다.
 - 우선은 추론까지 해서, 제대로 이뤄지는지는 확인해봐야할 듯 싶다.
 - 까먹을 뻔 했는데, 첫날 학습 parquet에는 "토요일"과 "평일"의 "시간대 그룹"에 해당하는 각각의 데이터를 1개씩 꼭 넣어줘야 한다.
 - gradient가 흘러야 나중에 다른 요일_시간대 그룹의 데이터가 들어오더라도 모델이 당황하지 않을 것이다.
 - 의미 있는 data를 넣을까 하다가 그래봤자 row 1개인 수준이라, 그냥 dummy를 입력하기로 했다. insertDummyRows.py 추가.

---
### 2025-05-21
#### 진행 상황
 - 학습 -> 추론 -> 자가반성 -> 복습 -> 추론 -> ... loop를 타려고 한 의도였다.
 - 아주 기본적인 상식을 순간 잊었다. 이대로라면 자가반성/복습 모델 사이에 dim 차이가 발생해 누적 자체가 불가능해진다.
 - loss 구조는 현재로서는 prev pred elapsed가 존재할 때만 작용하니, 그냥 0으로라도 16차원을 넣어주는게 맞지 않을까?
 - 날짜때문에 헷갈려 미치겠다. 5월 7일 새벽에는 5월 7일 ETA Table을 예측하는거다. 그걸 위해서 5월 5일 mean과 5월 6일 raw를 이용해서 학습하고, 학습이 끝나면 5월 7일 예측정보와 5월 6일 mean을 이용해서 학습한다.
 - 일단 loop 구조를 위한 5개의 코드 완성.
  + mean 생성 (2025-05-05)
  + buildReplayParquet (2025-05-07)
  + insertDummyRows
  + trainFirstETA --mode replay (2025-05-07)
  + mean 생성 (2025-05-06)
  + generateFirstETA (2025-05-07) [[
  + buildReviewParquet (2025-05-08)
  + trainFirstETA --mode self_review (2025-05-08)
  + buildReplayParquet (2025-05-08)
  + trainFirstETA --mode replay (2025-05-08)
  + mean 생성 (2025-05-07)
  + generateFirstETA (2025-05-08)
  + ]] ... loop

---
### 2025-05-22
#### 진행 상황
 - 추론값이 음수가 나오는 괴상한 상황이 벌어져서 모델을 조금 손봤다. 마지막 final부분에서 ReLU를 안 먹인 탓이 아닐까 싶다.
 - ReLU를 마지막에 추가해줬더니 갑자기 log var이 터졌는지 13에폭부터 NaN을 띄운다. clamp 걸어서 안정화.
 - 이번엔 전부 0에 수렴한다. 다시 ReLU는 빼고 clamp만 걸고 한번 상태를 보자.
 - sigmoid까지 끼워봤는데 답이 없다. 아예 최초 학습 한정으로 6일치 데이터를 넣고 해보는게 방법이 되지 않을까 싶다. 우선은... 숨이 막힌다.
 - 모델을 이리저리 만져보면서 디버깅을 해보는데, 우선 학습 자체는 이상이 없는것으로 보인다. min, max, mean값 모두 정상적으로 출력된다.
 - 높은 확률로 추론이 문제다.
 - 와. 추론코드에서 나도 모르게 RNN-like 추론을 시키고 있었던게 핵심 원인이었다. 현재로선 거의 실제와 유사하게 추론이 이뤄진다.
 - 세상에... 뭔가 진짜 큰 것을 해낸 느낌이다. 물론 아직 멀었다. self-review까지 해보고, 실제랑 차이가 크게 벌어지진 않는지. 학습을 쌓아도 유지되는지까지가 관건이다.
 - 계속 해보자.
 - self-review용 parquet 생성기 초안 완성. 일단 돌려본다.
 - 논리는 거의 다 맞는데 초장에 에러가 났다. 디버깅...
 - 추론 코드 및 파케이 생성 코드에서 중대한 오류를 발견했다. mean fallback이 제대로 이뤄지고 있지 않았다. (중복 정규화 발생 가능)
 - 즉시 수정했고, self-review용 파케이 생성기는 일단 완성되긴 했다. loop를 돌린 이후 육안으로 비교해보도록 하겠다.
 - 갑자기 또 망가지고 고쳐지고를 반복하는데 진짜 이유를 알 수가 없다. 확실히 짚고 넘어가야겠다.
 - 진짜 이유를 알 수가 없다. 학습 제대로 되는데???;; 일단 train은 인수 순서는 정말 관계 없겠지만 혹시를 대비해 --mode, --date 순으로 집어넣어보자.
 - 혹시 하고 시도해봤는데, 아마 dummy row에 관한 문제로 추정된다.
 - 5월 7일에 대한 추론은 다 실패하고 가끔씩 성공하는데 (운빨), 5월 11일 (일요일)로, 즉 첫 학습된 요일과 똑같은 요일로 추론해보니까 잘 된다.
 - 이제 다음과 같은 순서로 해결을 시도해본다.
  + dummy row 개수 단순 증가 (현재 16개인데, 최소 10배 이상)
  + 이걸로 해결이 안 되면, dummy가 아닌 실제 의미 있는 값으로 시도 (평일과 토요일 데이터를 일부 뽑아와서 첫날 학습)
  + 여기까지도 해결이 안 되면, 그냥 첫날 학습을 n일치 데이터를 한번에 모아서 시도하고, 그 다음부터 1일치 loop를 돌게 가공.

---
### 2025-05-23
#### 진행 상황
 - 우선 dummy를 320개로 늘려서 테스트를 해볼 것. 이걸로 안 된다면 그냥 바로 6일치나 13일치 데이터를 모아 최초 학습을 돌리는 방향으로 진행하겠다.
 - 분포를 생각하면 6일치 데이터를 모아 학습을 돌려보는게 좋다고 생각된다.
 - 원인을 확정했다. dummy 부족에 따른 gradient 흐름 불충분이었다.
 - 320개로 늘린 뒤 테스트해보니 값이 꽤 정상적으로 나온다. (3회 실험 모두 안정적)
 - 이제 고민. 초회학습을 6일치로 늘릴지, 아님 현행으로 두고 그냥 진행할지다.

 - 음... 잘 나오긴 한다.
 - 근데 모델 확장에 대한 욕심과, 일단 만족한 뒤 2차 모델로 넘어가느냐에 대한 고민이 된다. 아직 시간은 그래도 좀 있다. 3주정도?
 - 우선 추론기 병렬화는 시도해봤는데 GPU사용률만 높아지고 컨텍스트 스위칭 때문인지 뭔지 오히려 속도는 느려졌다. 그냥 직렬 유지해야할듯. 어차피 3~4분 내외에 끝난다.
 - 3일정도만 더 1차에 투자를 해보자. baseline이 안정화될수록 실시간 추론도 더더욱 쉬워지는 법이다.
 - 실험을 거친다. mean feature 자체 1개 더 추가 (stdid_ord 귀속된 "이전 정류장까지 elapsed 또는 이전 정류장~현 정류장 간격)
 - mean_elapsed, mean_interval 하나 더 추가 (stdid_ord별 mean, stop_id별 mean (현재는 해당 stdid_ord의 group별 mean이다.))
 - 실험적으로 최초 6일을 한번에 parquet으로 가공하여 학습에 투입.
 - mean feature 추가를 위해 모델을 분석하던 도중 현재 모델의 문제점을 발견했다. ord ratio와 mean elapsed가 완벽히 route에 종속되지 않는 문제였다.
 - 현재 해결 중.
 - 수정 완료. 종속성 부여.